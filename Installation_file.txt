Hadoop installation steps


1.sudo apt-get update
2.sudo apt-get install openssh-server
(if resources locked than apply below commands
2.1 sudo rm /var/lib/apt/lists/lock
2.2 sudo rm /var/cache/apt/archives/lock
2.3 sudo rm /var/lib/dpkg/lock
)
3 ssh-keygen(press enter untill you back to $)
4.cat ~/.ssh/id_rsa.pub >>  ~/.ssh/authorized_keys
5. ssh localhost
6. Press ctrl+d to exit ssh
7. Install Java 
sudo apt-get install openjdk-8-jdk
8. check java version 
java -version
9. set JAVA_HOME 
gedit .bashrc
==================Paste code in file
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
==================
10.exec bash
11. Download Hadoop ***.tar.gz and move to /usr/local
sudo mv hadoop-2.4.1.tar.gz /usr/local
12. Extract tar file 
cd /usr/local/
sudo tar -xvzf hadoop-x.x.x.tar.gz
cd
13.  change ownership (my system name is ubuntu:ubuntu)
sudo chown -R ubuntu:ubuntu hadoop
14.Set environment of Hadoop
gedit .bashrc
=====================================
export HADOOP_PREFIX=/usr/local/hadoop
export PATH=$PATH:$HADOOP_PREFIX/bin
export HADOOP_HOME=/usr/local/hadoop 
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export HADOOP_COMMON_HOME=$HADOOP_HOME 
export HADOOP_HDFS_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
======================================
15. exec bash
16. cd /usr/local/hadoop/etc/hadoop
cp mapred-site.xml.template mapred-site.xml
17. write below text in mapred-site.xml
======================================
<configuration>
<property> 
      <name>mapreduce.framework.name</name> 
      <value>yarn</value> 
   </property>
</configuration>
======================================
18.edit core-site.xml
======================================
<configuration>

   <property> 
      <name>fs.default.name</name> 
      <value>hdfs://localhost:9000</value> 
   </property>
</configuration>
======================================
19. edit hadoop-env.sh
======================================
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
======================================
20. edit hdfs-site.xml
======================================

<configuration>

<property> 
      <name>dfs.replication</name> 
      <value>1</value> 
   </property> 
<property>
<name>dfs.permissions</name>
<value>false</value>
</property>
   <property> 
      <name>dfs.name.dir</name> 
      <value>file:////home/ubuntu/namenode </value> 
   </property> 
   <property> 
      <name>dfs.data.dir</name>
      <value>file:////home/ubuntu/datanode </value > 
   </property>
</configuration>
======================================
21. edit yarn-site.xml
======================================
<configuration>

<!-- Site specific YARN configuration properties -->

  <property> 
      <name>yarn.nodemanager.aux-services</name> 
      <value>mapreduce_shuffle</value> 
   </property>
   <property>
  <name>yarn.nodemanager.vmem-check-enabled</name>
  <value>false</value>
  <description>Whether virtual memory limits will be enforced for containers.</description>
</property>
<property>
  <name>yarn.nodemanager.vmem-pmem-ratio</name>
  <value>5</value>
  <description>Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.</description>
</property>
<property> 
      <name>yarn.scheduler.minimum-allocation-mb</name> 
      <value>1024</value> 
   </property>
<property> 
      <name>yarn.scheduler.maximum-allocation-mb</name> 
      <value>7024</value> <!-- Choose as per your ram n your system -->
   </property>
<property> 
      <name>yarn.scheduler.minimum-allocation-vcores</name> 
      <value>1</value> 
   </property>
<property> 
      <name>yarn.scheduler.maximum-allocation-vcores</name> 
      <value>4</value> 
   </property>
</configuration>
======================================
22. hdfs namenode -format
23. jps
24. start-dfs.sh
25. start-yarn.sh
(you can also use start-all.sh command but it is deprecated)
26. for stop hadoop
stop-dfs.sh
27. stop-yarn.sh